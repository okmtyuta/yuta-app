---
title: Learning records from the 2024-06-23
postedAt: 2024-06-23
slug: '6'
description: None
genre: mathematics
tags: []
---

### Transformer: self-attention mechanism and key-query-value attention mechanism

Let $W = \left( w_1, \cdots, w_n \right)$ be an input tokens to the encoder of Transformer, and for token $w$, let $e_w$ be the $D$-dimensional embedding of $w$. Since the embedding sequence $\left( e_{w_1}, \cdots, e_{w_n} \right)$ of $W$ does not include information about the position and order of tokens, any permutation of $W$ will be processed as a same input as $W$. To solve this problem, we add **_positional encodings_** to embeddings of input tokens. The $D$-dimensional positional encoding $p_i$ corresponding to $i$-th token $w_i$ in $W$ is determined by

$$
\begin{align*}
    p_{i, 2k+1} &= \sin\left( \dfrac{ i }{ 10000^{2k / D} } \right),\\
    p_{i, 2k+2} &= \cos\left( \dfrac{ i }{ 10000^{2k / D} } \right),
\end{align*}
$$

where $p_{i, j}$ is the $j$-th element of $p_i$ and $k = 0, 1, \cdots, D / 2 - 1$. Then, the embedding $x_i$ of $w_i$ into model is calculated as follows:

$$
\begin{align*}
    x_i = \sqrt{D} e_{w_i} + p_i.
\end{align*}
$$

The **_self-attention mechanism_** is responsible for contextualization while taking the importance of tokens into account. The **_key-query-value attention mechanism_** employed by Transformer, three different embeddings, key, query and value, are calculated for input embeddings. The key-query-value attention mechanism includes three $D\times D$ matrix, $W_q$, $W_k$ and $W_v$, to calculate query-embedding, key-embedding and value-embedding, respectively. Hence, for a $D$-dimensional embedding sequence $h_1, \cdots, h_N$, the query-embedding $q_i$, the key-embedding $k_i$, and the value-embedding $v_i$ are calculated as follows:

$$
\begin{align*}
    q_i = W_q h_i,\ k_i = W_k h_i,\ v_i = W_v h_i.
\end{align*}
$$

Then, the relevance score $s_{i, j}$ of the $j$-th token to the $i$-th token is calculated by inner product as follows:

$$
\begin{align*}
    s_{i,j} = \dfrac{ q_i^T k_j }{ \sqrt{D} }.
\end{align*}
$$

Using the weight

$$
\begin{align*}
    \alpha_{i, j} = \dfrac{ \exp \left( s_{i, j} \right) }{ \sum_{j^\prime = 1}^N \exp \left( s_{i, j^\prime} \right) }
\end{align*}
$$

which is normalized with relevance score $s_{i,j}$ and the softmax function, the $i$-th output of key-query-value attention mechanism is represented as follows:

$$
\begin{align*}
    o_i = \sum_{j = 1}^N \alpha_{i, j} v_j
\end{align*}
$$
